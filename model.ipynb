{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "angles = []\n",
    "samples = []\n",
    "correction = 0.25 # this is a parameter to tune\n",
    "def readFromDir(dir_name, if_windows):\n",
    "    if if_windows:\n",
    "        split_str = '\\\\'\n",
    "    else:\n",
    "        split_str = '/'\n",
    "    data_pd = pd.read_csv(dir_name+'/driving_log.csv', header=None)\n",
    "    lines = data_pd.values\n",
    "    for line in lines:\n",
    "        center_image = dir_name+'/IMG/'+line[0].split(split_str)[-1]\n",
    "        left_image = dir_name+'/IMG/'+line[1].split(split_str)[-1]\n",
    "        right_image = dir_name+'/IMG/'+line[2].split(split_str)[-1]\n",
    "#         center_image = cv2.imread(name)\n",
    "        center_angle = float(line[3])\n",
    "#         images.append(center_image)\n",
    "#         angles.append(center_angle)\n",
    "        samples.append((center_image, center_angle))\n",
    "        samples.append((left_image, center_angle+correction))\n",
    "        samples.append((right_image, center_angle-correction))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "42351\n",
      "('../behavior/behavior1/IMG/center_2017_12_16_11_41_39_478.jpg', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# get all images and angles in center \n",
    "behavior1 = '../behavior/behavior1'\n",
    "behavior2 = '../behavior/behavior2' \n",
    "behavior3 = '../behavior/behavior3'\n",
    "readFromDir(behavior1, if_windows=True)\n",
    "readFromDir(behavior2, if_windows=True)\n",
    "readFromDir(behavior3, if_windows=False)\n",
    "# print(len(images))\n",
    "print(len(angles))\n",
    "# print(images[0].shape)\n",
    "print(len(samples))\n",
    "print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(42351,)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAENCAYAAADkNanAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncDAzs4LDwczMCv8fp4khPEoFDA0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f617e8c2ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train = np.array(samples)[:,1]\n",
    "y_train = y_train.astype(float)\n",
    "print(y_train.dtype)\n",
    "print(y_train.shape)\n",
    "hist, bins = np.histogram(y_train, bins=20)\n",
    "print()\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.title(r'train size summary', fontsize=19)\n",
    "plt.savefig(\"train_size.jpg\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_array = np.array(images)\n",
    "# y_array = np.array(angles)\n",
    "# X_, y_ = shuffle(X_array, y_array)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# data_file = \"data.p\"\n",
    "# data_var = {\"X_train\":X_train, \"X_test\":X_test, \"y_train\":y_train, \"y_test\":y_test}\n",
    "\n",
    "# with open(data_file, mode='wb') as f:\n",
    "#     pickle.dump(data_var, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform x y direction, input original rgb image\n",
    "def transform_image(image, angle):\n",
    "    trans_range = 150\n",
    "    tx = trans_range*(np.random.uniform()-0.5)\n",
    "    ty = 10*(np.random.uniform()-0.5)\n",
    "    angle = angle + tx/trans_range*0.5  # max adjust angle 0.5\n",
    "    rows,cols,_ = image.shape\n",
    "    M = np.float32([[1,0,tx],[0,1,ty]])\n",
    "    image = cv2.warpAffine(image,M,(cols,rows))\n",
    "    return image,angle\n",
    "\n",
    "# Horizontal Flip\n",
    "def flip_image(image, angle):\n",
    "    # Horizontal Flip\n",
    "    image = cv2.flip(image, 1)\n",
    "    return image, -angle\n",
    "\n",
    "# adjust brightness\n",
    "def bright_image(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = v.astype(np.float)\n",
    "    v *= (np.random.uniform()+0.2)\n",
    "    v[v>255] = 255\n",
    "    v[v<0] = 0\n",
    "    v = v.astype(np.uint8)\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    image_hsv = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2RGB)\n",
    "    return image_hsv\n",
    "\n",
    "# input rgb image\n",
    "def preprocess_image(image):\n",
    "    rows,cols,_ = image.shape\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "#     image = cv2.equalizeHist(image)\n",
    "    image = image[50:rows-20, 0:cols]\n",
    "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    image = image/255.0 - 0.5\n",
    "#     image = np.expand_dims(image, axis=2)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33880\n",
      "8471\n"
     ]
    }
   ],
   "source": [
    "samples = shuffle(samples)\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "print(len(train_samples))\n",
    "print(len(validation_samples))\n",
    "\n",
    "def generator_train(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = batch_sample[0]\n",
    "#                 center_image = cv2.imread(name)\n",
    "                image = np.asarray(Image.open(name))\n",
    "                angle = float(batch_sample[1])\n",
    "                image, angle = transform_image(image, angle)\n",
    "                if np.random.choice([True, False]):\n",
    "                    image, angle = flip_image(image, angle)\n",
    "                image = bright_image(image)\n",
    "                image = preprocess_image(image)\n",
    "                images.append(image)\n",
    "                angles.append(angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_ = np.array(images)\n",
    "            y_ = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_, y_)\n",
    "\n",
    "            \n",
    "def generator_valid(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = batch_sample[0]\n",
    "#                 center_image = cv2.imread(name)\n",
    "                image = np.asarray(Image.open(name))\n",
    "                angle = float(batch_sample[1])\n",
    "                image = preprocess_image(image)\n",
    "                images.append(image)\n",
    "                angles.append(angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_ = np.array(images)\n",
    "            y_ = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_, y_)            \n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator_train(train_samples, batch_size=32)\n",
    "validation_generator = generator_valid(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 16)   64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 32)   12832       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 28, 28, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 32)   1056        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 14, 14, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 10, 10, 64)   51264       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10, 10, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 5, 5, 64)     4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 5, 5, 64)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 128)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3200)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3200)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          409728      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 479,233\n",
      "Trainable params: 479,233\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tzyhpcom/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/tzyhpcom/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., verbose=1, callbacks=[<keras.ca..., steps_per_epoch=33880, epochs=10, validation_steps=8471)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33879/33880 [============================>.] - ETA: 0s - loss: 0.0430Epoch 00001: val_loss improved from inf to 0.02614, saving model to model-01-0.0261.hdf5\n",
      "33880/33880 [==============================] - 5668s 167ms/step - loss: 0.0430 - val_loss: 0.0261\n",
      "Epoch 2/10\n",
      "33879/33880 [============================>.] - ETA: 0s - loss: 0.0299Epoch 00002: val_loss improved from 0.02614 to 0.02328, saving model to model-02-0.0233.hdf5\n",
      "33880/33880 [==============================] - 5664s 167ms/step - loss: 0.0299 - val_loss: 0.0233\n",
      "Epoch 3/10\n",
      "33879/33880 [============================>.] - ETA: 0s - loss: 0.0280Epoch 00003: val_loss improved from 0.02328 to 0.02082, saving model to model-03-0.0208.hdf5\n",
      "33880/33880 [==============================] - 5661s 167ms/step - loss: 0.0280 - val_loss: 0.0208\n",
      "Epoch 4/10\n",
      "33879/33880 [============================>.] - ETA: 0s - loss: 0.0271Epoch 00004: val_loss did not improve\n",
      "33880/33880 [==============================] - 5664s 167ms/step - loss: 0.0271 - val_loss: 0.0212\n",
      "Epoch 5/10\n",
      " 1222/33880 [>.............................] - ETA: 1:20:51 - loss: 0.0268"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Lambda, Concatenate\n",
    "from keras.layers import Cropping2D, Input\n",
    "from keras.layers import Reshape\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import tensorflow\n",
    "import cv2\n",
    "\n",
    "lamda_w = 1e-4\n",
    "\n",
    "# model = Sequential()\n",
    "input1 = Input(shape=(64, 64, 3))\n",
    "         \n",
    "conv1 = Conv2D(16, (1, 1), strides=(1, 1), padding='valid', kernel_regularizer=regularizers.l2(lamda_w))(input1)\n",
    "conv1 = Activation('elu')(conv1)\n",
    "conv1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(conv1)\n",
    "\n",
    "conv2 = Conv2D(32, (5, 5), strides=(1, 1), padding='valid', kernel_regularizer=regularizers.l2(lamda_w))(conv1)\n",
    "conv2 = Activation('elu')(conv2)\n",
    "conv2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(conv2)\n",
    "\n",
    "conv3 = Conv2D(32, (1, 1), strides=(1, 1), padding='valid', kernel_regularizer=regularizers.l2(lamda_w))(conv2)\n",
    "conv3 = Activation('elu')(conv3)\n",
    "\n",
    "conv4 = Conv2D(64, (5, 5), strides=(1, 1), padding='valid', kernel_regularizer=regularizers.l2(lamda_w))(conv3)\n",
    "conv4 = Activation('elu')(conv4)\n",
    "conv4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(conv4)\n",
    "\n",
    "conv5 = Conv2D(64, (1, 1), strides=(1, 1), padding='valid', kernel_regularizer=regularizers.l2(lamda_w))(conv4)\n",
    "conv5 = Activation('elu')(conv5)\n",
    "\n",
    "merge1 = Concatenate()([conv4, conv5])\n",
    "flat1 = Flatten()(merge1)\n",
    "flat1 = Dropout(0.5)(flat1)\n",
    "\n",
    "dense1 = Dense(128, kernel_regularizer=regularizers.l2(lamda_w))(flat1)\n",
    "dense1 = Activation('elu')(dense1)\n",
    "dense1 = Dropout(0.5)(dense1)\n",
    "\n",
    "output = Dense(1)(dense1)\n",
    "\n",
    "model = Model(inputs=input1, outputs=output)\n",
    "\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='mse', optimizer=adam)\n",
    "model.summary()\n",
    "\n",
    "filepath=\"model-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch= len(train_samples), \\\n",
    "                                     validation_data=validation_generator, \\\n",
    "                                     nb_val_samples=len(validation_samples), \\\n",
    "                                     nb_epoch=10, verbose=1, callbacks=callbacks_list)\n",
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# image1 = cv2.imread(samples[0][0])\n",
    "# image2 = Image.open(samples[0][0])\n",
    "# plt.figure()\n",
    "# plt.imshow(image1)\n",
    "# plt.figure()\n",
    "# plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# history_object = model.fit_generator(train_generator, samples_per_epoch =\n",
    "#     len(train_samples), validation_data = \n",
    "#     validation_generator,\n",
    "#     nb_val_samples = len(validation_samples), \n",
    "#     nb_epoch=5, verbose=1)\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.savefig(\"loss.jpg\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
